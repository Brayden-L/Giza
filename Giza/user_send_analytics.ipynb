{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "import grequests # You will get errors if grequests is not above requests\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from pandarallel import pandarallel\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import cchardet\n",
    "import re\n",
    "\n",
    "import pyinputplus as pyip\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import math \n",
    "import string\n",
    "\n",
    "from unique_route_handling import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning) # Grequests is a monkeypatch and not intended to be used with jupyter. This silences an annoying userwarning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- IMPORT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_link = 'https://www.mountainproject.com/user/200180658/brayden-l'\n",
    "upload_type = 'tick'\n",
    "df_usend, _ = download_routelist(upload_type, upload_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- DATA CLEANSE AND STANDARDIZE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend = data_standardize(df_usend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- CREATE UNIQUE LIST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend_uniq = df_usend.drop_duplicates(subset=\"Route ID\")\n",
    "df_usend_uniq = user_uniq_clean(df_usend_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- GRADE HOMOGENIZATION AND ROUTE LENGTH CLEANUP ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend_uniq = route_length_fixer(df_usend_uniq, 'express')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_settings = ['letter', 'even_rand', 'flat', 'even_rand']\n",
    "df_usend_uniq = grade_homo(df_usend_uniq, *grade_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- SCRAPE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_usend_uniq \u001b[39m=\u001b[39m routescrape_syncro(df_usend_uniq)\n",
      "File \u001b[1;32mc:\\Users\\Brayden\\Desktop\\Giza\\Giza\\unique_route_handling.py:412\u001b[0m, in \u001b[0;36mroutescrape_syncro\u001b[1;34m(df_source, retries)\u001b[0m\n\u001b[0;32m    409\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m--> 412\u001b[0m stqdm\u001b[39m.\u001b[39mpandas(desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(1/5) Scraping Mainpages\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mRe Mainpage\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m df_source\u001b[39m.\u001b[39mcolumns: \u001b[39m# Creates column if it does not yet exist, otherwise it will try to download any that errored last attempt\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     df_source\u001b[39m.\u001b[39minsert(\u001b[39mlen\u001b[39m(df_source\u001b[39m.\u001b[39mcolumns),\u001b[39m'\u001b[39m\u001b[39mRe Mainpage\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stqdm' is not defined"
     ]
    }
   ],
   "source": [
    "df_usend_uniq = routescrape_syncro(df_usend_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- ANALYZE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend_uniq = extract_default_pitch(df_usend_uniq, par=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend_uniq = extract_tick_details(df_usend_uniq, par=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usend_uniq = tick_analysis(df_usend_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the now scraped dataframe to a pickle file\n",
    "df_usend_uniq.to_pickle('../Data_Archive/df_usend_archive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the new pickle file\n",
    "picklefile = open('../Data_Archive/df_usend_archive', 'rb')\n",
    "df_usend_uniq = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- MERGE UNIQUE DATA TO TICK LIST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge unique dataframe details to user data frame. This will delete the user data frame of length and rating information and replace it with that from the unique dataframe\n",
    "df_usendm = df_usend.copy() # We create a copy of the original df_usend to be consequently modified by the user. This leaves the original dataframe alone.\n",
    "df_usendm.drop(columns=['Rating', 'Length'], inplace=True)\n",
    "df_usendm = df_usendm.merge(df_usend_uniq[['Route ID', 'Pitches', 'Lead Ratio', 'Num Ticks', 'Num Tickers', 'OS Ratio', 'Mean Attempts To RP', 'Rating', 'Length']], how='left', on='Route ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- TAG NOTABLE SENDS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize columns\n",
    "\n",
    "df_usendm.insert(len(df_usendm.columns),'Flash/Onsight',None)\n",
    "df_usendm.insert(len(df_usendm.columns),'Worked Clean',None)\n",
    "df_usendm.insert(len(df_usendm.columns),'Grade Breakthrough',None)\n",
    "df_usendm.insert(len(df_usendm.columns),'Attempts',float('NaN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to tag important climbs, namely flash/onsights, worked clean routes and grade breakthroughs.\n",
    "\n",
    "# Tag climbs that were flash/onsight\n",
    "df_usendm.loc[df_usendm['Lead Style'].isin(CLEAN_SEND_FIRST), 'Flash/Onsight'] = True\n",
    "\n",
    "# Create column that flags climbs that were worked. There are three possibilities to consider. We want 1 and 2.\n",
    "# 1. Worked to clean send, no further sends.\n",
    "# 2. Worked to clean send, additional attempts.\n",
    "# 3. Sent clean first try, additional attempts.\n",
    "df_all_dupes = df_usendm[df_usendm.duplicated(subset=\"Route ID\", keep=False)] # First we filter for all duplicate entries.\n",
    "df_all_worked = df_all_dupes.groupby('Route ID').filter(lambda x: ~x['Lead Style'].isin(CLEAN_SEND_FIRST).any()) # Then we remove all groups which have a lead style of flash or onsight to eliminate group 3.\n",
    "df_worked_clean_rponly = df_all_worked[df_all_worked.groupby('Route ID')['Lead Style'].apply(lambda x: x.isin(CLEAN_SEND_WORKED))] # fell/hungs and TRs remain, so we take ticks from CLEAN_SEND_WORKED.\n",
    "df_worked_clean_earliest = df_worked_clean_rponly.loc[df_worked_clean_rponly.groupby('Route ID')['Date'].idxmin()] # Use only the earliest redpoint to correctly identify the first redpoint.\n",
    "df_usendm.loc[df_worked_clean_earliest.index.values, \"Worked Clean\"] = True\n",
    "\n",
    "# Flag grade breakthrough ticks\n",
    "dfbreakthr = df_usendm[(df_usendm['Flash/Onsight'] == True) | (df_usendm['Worked Clean'] == True)]\n",
    "breakthrough_indexes = dfbreakthr.groupby('Rating', observed=True)['Date'].idxmin().values\n",
    "df_usendm.loc[breakthrough_indexes, \"Grade Breakthrough\"] = True\n",
    "df_usendm.loc[breakthrough_indexes]\n",
    "\n",
    "# Count number of attempts to send\n",
    "# Assumes no style lead ticks are fell/hung\n",
    "# Assumes rp/pp with no prior tick history has one prior attempt\n",
    "# Counts clean ticks with multiple pitches as total attempts. It also counts fell/hung, and TR with multiple pitches as multiple attempts.\n",
    "# !!! This will falsely identify a single pitch climb broken into multiple pitches as two attempts, there isn't really a good way to detect this.\n",
    "df_worked_clean = df_all_worked.groupby('Route ID').filter(lambda x: x['Lead Style'].isin(CLEAN_SEND_WORKED).any()) # Filters out worked climbs that were never done clean.\n",
    "num_to_send = df_worked_clean.groupby('Route ID').apply(lambda x: count_attempt2rp(x, x.iloc[0]['Pitches']))\n",
    "num_to_send.rename('Attempts', inplace=True)\n",
    "matched_attempts = df_usendm[df_usendm['Worked Clean'] == True].merge(num_to_send, on=\"Route ID\", how=\"left\")\n",
    "matched_attempts.index = df_usendm[df_usendm['Worked Clean'] == True].index # I'm dumb and this is the best way I could find to get my index to remain\n",
    "df_usendm.loc[matched_attempts.index, \"Attempts\"] = matched_attempts.iloc[:,-1]\n",
    "df_usendm.loc[df_usendm['Attempts'] == 1, 'Attempts'] = 2 # This assumes rp with 1 pitch and no prior ticks had one prior attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- ANALYZE FOR NOTABLE ELEMENTS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User led something rarely led\n",
    "df_bold_leads = df_usendm[(df_usendm['Num Ticks'] >= 30) & (df_usendm['Lead Ratio'] < 0.4) & (df_usendm['Style'] == 'Lead') & (df_usendm['Route Type'] != 'Boulder')].sort_values(by='Lead Ratio', ascending=False)\n",
    "\n",
    "# User onsighted something rarely onsighted\n",
    "df_impressive_OS = df_usendm[(df_usendm['Num Ticks'] >= 30) & (df_usendm['OS Ratio'] < 0.35) & (df_usendm['Flash/Onsight'] == True) & (df_usendm['Route Type'] != 'Boulder')].sort_values(by='OS Ratio')\n",
    "\n",
    "# User fell on something rarely fallen on\n",
    "df_woops_falls = df_usendm[(df_usendm['Num Ticks'] >= 30) & (df_usendm['OS Ratio'] > 0.8) & (df_usendm['Style'] == 'Lead') & (df_usendm['Lead Style'] == 'Fell/Hung') & (df_usendm['Route Type'] != 'Boulder')].sort_values(by='OS Ratio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- FILTER FOR VIS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a copy of the user modified dataframe, which we will refer to as the filtered dataframe\n",
    "df_usendf = df_usendm.copy()\n",
    "\n",
    "# Filters, these would be sliders and options in an interactive plot\n",
    "roped_grade_min = '5.10a'\n",
    "boulder_grade_min = 'V0'\n",
    "\n",
    "# We slice the dataframe into a route and a boulder type. Each type will receive it's own independent filtering.\n",
    "\n",
    "df_usendf_r = df_usendf[df_usendf['Rating'].isin(YDS_GRADES_FULL[YDS_GRADES_FULL.index(roped_grade_min):])] # the : here is key, it makes it so that it includes all grades above the min included.\n",
    "df_usendf_b = df_usendf[df_usendf['Rating'].isin(V_GRADES_FULL[V_GRADES_FULL.index(boulder_grade_min):])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visualization settings accordant to modifications and filters.\n",
    "\n",
    "if grade_settings[0] == 'letter':\n",
    "    ryaxorder = YDS_GRADES_LETTER\n",
    "if grade_settings[0] == 'sign':\n",
    "    ryaxorder = YDS_GRADES_SIGN\n",
    "\n",
    "if grade_settings[2] == 'flat':\n",
    "    byaxorder = V_GRADES_FLAT\n",
    "if grade_settings[2] == 'sign':\n",
    "    byaxorder = V_GRADES_SIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- SPLIT INTO ROPED AND BOULDER SUBSETS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of clean sends for analysis\n",
    "\n",
    "df_usendf_r.loc[df_usendf_r['Lead Style'].isin(CLEAN_SEND_FIRST), \"Attempts\"] = '' # Optionally change attempts from blank to 1 or other\n",
    "df_clean_sends_r = df_usendf_r[(df_usendf_r['Lead Style'].isin(CLEAN_SEND))]\n",
    "# df_clean_sends_r = df_clean_sends_r.loc[df_clean_sends_r.groupby('Route ID')['Date'].idxmin()] # Optionally ignore subequent clean sends\n",
    "df_clean_sends_r['Date Formatted'] = df_clean_sends_r['Date'].dt.date\n",
    "\n",
    "df_usendf_b.loc[df_usendf_b['Lead Style'].isin(CLEAN_SEND_FIRST), \"Attempts\"] = '' # Optionally change attempts from blank to 1 or other\n",
    "df_clean_sends_b = df_usendf_b[(df_usendf_b['Style'].isin(CLEAN_SEND))]\n",
    "# df_clean_sends_b = df_clean_sends_b.loc[df_clean_sends_r.groupby('Route ID')['Date'].idxmin()] # Optionally ignore subequent clean sends\n",
    "df_clean_sends_b['Date Formatted'] = df_clean_sends_b['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_clean_sends_r, y=\"Rating\", orientation='h', category_orders={\"Rating\": ryaxorder[::-1]}, text='Attempts', custom_data=['Route', 'Date Formatted', 'Location', 'Length', 'Avg Stars']) # The [::-1] is an inverse slice\n",
    "fig.update_layout(font={'family':'Courier New', 'color':'black', 'size':20}, title={'text':'<b>Climbing Pyramid</b>', 'x':0.5, 'font_size':30}, xaxis={'title': 'Number of Routes Sent'}, yaxis={'title': 'Grade'}, paper_bgcolor='#ece5dc', plot_bgcolor='#F5D3A5', bargap=0)\n",
    "fig.update_traces(marker_color='#7A4F25', textposition = \"inside\", textfont={\"color\": 'White', \"size\": 12, \"family\": 'Arial Black'},  hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')\n",
    "# fig.update_traces(marker_color=list(map(lambda x: '#7A4F25' if (x=='') else '#bf9315', df_clean_sends['Attempts'])), textposition = \"inside\",  hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_clean_sends_r, \"Date\", \"Rating\", category_orders={\"Rating\": ryaxorder[::-1]}, text='Attempts', custom_data=['Route', 'Date Formatted', 'Location', 'Length', 'Avg Stars'])\n",
    "fig.update_layout(font={'family':'Courier New', 'color':'black', 'size':20}, title={'text':'<b>Send by Date</b>', 'x':0.5, 'font_size':30}, xaxis={'title': 'Date'}, yaxis={'title': 'Grade'}, paper_bgcolor='#ece5dc', plot_bgcolor='#F5D3A5', bargap=0)\n",
    "fig.update_traces(marker_symbol='square', marker_color='#7A4F25', marker_size=20, marker_line_width=2, marker_line_color='black', textfont={\"color\": 'White', \"size\": 12}, hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_clean_sends_b, y=\"Rating\", orientation='h', category_orders={\"Rating\": byaxorder[::-1]}, text='Attempts', custom_data=['Route', 'Date Formatted', 'Location', 'Length', 'Avg Stars']) # The [::-1] is an inverse slice\n",
    "fig.update_layout(font={'family':'Courier New', 'color':'black', 'size':18}, title={'text':'<b>Climbing Pyramid</b>', 'x':0.5, 'font_size':30}, xaxis={'title': 'Number of Problems Sent'}, yaxis={'title': 'Grade'}, paper_bgcolor='#ece5dc', plot_bgcolor='#F5D3A5', bargap=0)\n",
    "fig.update_traces(marker_color='#7A4F25', textposition = \"inside\", textfont={\"color\": 'White', \"size\": 12, \"family\": 'Arial Black'},  hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')\n",
    "# fig.update_traces(marker_color=list(map(lambda x: '#7A4F25' if (x=='') else '#bf9315', df_clean_sends['Attempts'])), textposition = \"inside\",  hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_clean_sends_b, \"Date\", \"Rating\", category_orders={\"Rating\": byaxorder[::-1]}, text='Attempts', custom_data=['Route', 'Date Formatted', 'Location', 'Length', 'Avg Stars'])\n",
    "fig.update_layout(font={'family':'Courier New', 'color':'black', 'size':20}, title={'text':'<b>Send by Date</b>', 'x':0.5, 'font_size':30}, xaxis={'title': 'Date'}, yaxis={'title': 'Grade'}, paper_bgcolor='#ece5dc', plot_bgcolor='#F5D3A5', bargap=0)\n",
    "fig.update_traces(marker_symbol='square', marker_color='#7A4F25', marker_size=20, marker_line_width=2, marker_line_color='black', textfont={\"color\": 'White', \"size\": 12}, hovertemplate='Name: %{customdata[0]}<br>Date: %{customdata[1]}<br>Location: %{customdata[2]}<br>Length: %{customdata[3]}ft<br>Avg Stars: %{customdata[4]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69c68274f8e9ce57f21b9f4f0cd989e216f773741f6e1c7e6f7ea408f78a94ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
