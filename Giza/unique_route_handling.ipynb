{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pyinputplus as pyip\n",
    "import requests\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import math \n",
    "import string\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other files\n",
    "%run climbconstants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_routelist(upload_type, link):\n",
    "    \"\"\"\n",
    "    Downloads a tick or todo list from the MP website.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    upload_type : str {todo, tick} \n",
    "        A given user has a todo and a tick list. Choose one.\n",
    "    link : str\n",
    "        Link to user. Of format - https://www.mountainproject.com/user/########/name/\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : df\n",
    "        Downloaded dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(f'{link}/{upload_type}-export')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardize(df_source):\n",
    "    \"\"\"\n",
    "    Basic data cleanup for route df. Assigns correct datatype, creates unique route ID column, prepares \"rating\" for further analysis, handles bad data in \"route type\".\n",
    "    Input df, return df.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Some cleanup is only necessary on the tick list, and not the todo list. Only the tick list has a Date column.\n",
    "    if 'Date' in df_source.columns:\n",
    "        df_source['Date'] = pd.to_datetime(df_source['Date']) # 'Date' to datetype\n",
    "        df_source.rename(columns={'Pitches': 'Pitches Ticked'}, inplace=True) # Pitches relabeled to Pitches Ticked\n",
    "        df_source['Notes'] = df_source['Notes'].apply(lambda x: str(x).replace('&#39;',\"'\")) # Apostrophe's are html coded in the notes section for some reason.\n",
    "\n",
    "    # Remove all aid, ice, snow, TR only,and trad/boulder climbing route types as they are not relevant.\n",
    "    df_source = df_source[df_source['Route Type'].str.contains(r'Aid|Ice|Snow') != True]\n",
    "    df_source = df_source[df_source['Route Type'].str.fullmatch(r'TR') != True] #if this is just a partial match it will detech \"trad\" too!\n",
    "    df_source = df_source[df_source['Route Type'].str.contains(r'Trad') & df_source['Route Type'].str.contains(r'Boulder') != True]\n",
    "\n",
    "    # \"trad, sport\" goes to \"trad\". If it uses gear it's trad!\n",
    "    df_source.loc[df_source['Route Type'].str.contains(r'Trad') & df_source['Route Type'].str.contains(r'Sport'), 'Route Type'] = 'Trad'\n",
    "\n",
    "    # \"x, alpine\" and \"x, tr\" goes to \"x\" Alpine and tr tags are not useful.\n",
    "    def rem_route_el_from_list(ousted, seperator):\n",
    "        el_rem_subset = df_source['Route Type'].str.contains(ousted) == True\n",
    "        df_source.loc[el_rem_subset, 'Route Type'] = df_source[el_rem_subset]['Route Type'].apply(lambda row: [val for val in row.split(seperator) if val != ousted]).apply(lambda x: \", \".join(x))\n",
    "\n",
    "    rem_route_el_from_list('Alpine', ', ')\n",
    "    rem_route_el_from_list('TR', ', ')\n",
    "\n",
    "    routetype_cat = CategoricalDtype(categories=ROUTE_TYPES)\n",
    "    df_source['Route Type'] = df_source['Route Type'].astype(routetype_cat)\n",
    "    \n",
    "    # Extract route unique identifier from URL and create a new column for it.\n",
    "    if 'Route ID' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Route ID','')\n",
    "    df_source['Route ID'] = df_source['URL'].apply(lambda x: int(x.split('/')[4]))\n",
    "\n",
    "    # Change YDS-Vgrade combos to just Vgrade. They are most likely boulders, so a bouldering grade is relevant.\n",
    "    subset = df_source['Rating'].apply(lambda row: [val for val in row.split() if val in V_GRADES_FULL]).astype(bool)  & df_source['Rating'].apply(lambda row: [val for val in row.split() if val in YDS_GRADES_FULL]).astype(bool) == True\n",
    "    df_source.loc[subset, 'Rating'] = df_source[subset]['Rating'].apply(lambda x: x.split()[1])\n",
    "\n",
    "    # Seperate risk rating to new column\n",
    "    if 'Rating' not in df_source.columns:\n",
    "        df_source.insert(df_source.columns.get_loc('Rating')+1,'Risk','')\n",
    "    risk_cat = CategoricalDtype(categories=RISK_GRADES, ordered=True)\n",
    "    df_source['Risk'] = df_source['Rating'].apply(lambda row: [val for val in row.split() if val in RISK_GRADES]).apply(lambda x: \"\".join(x)).astype(risk_cat)\n",
    "    # Reduce Rating column to just rating\n",
    "    rating_cat = CategoricalDtype(categories=GRADES_SUPER)\n",
    "    df_source['Rating'] = df_source['Rating'].apply(lambda row: [val for val in row.split()][0]).astype(rating_cat)\n",
    "\n",
    "    # Create original rating and length archive to compare against or undo changes.\n",
    "    if 'Original Rating' not in df_source.columns:\n",
    "        df_source.insert(df_source.columns.get_loc('Rating'),'Original Rating',df_source['Rating'])\n",
    "\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_length_fixer(df_source, input_type):\n",
    "    \"\"\"\"\n",
    "    Fixes route length outliers and missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_source : df\n",
    "        df of routes\n",
    "    input_type : str {\"express\", \"manual\"}\n",
    "        Use default values or input manually\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_source : df\n",
    "        Original df with fully defined route lengths\n",
    "    \"\"\"\n",
    "### Handle route length outliers and NaNs\n",
    "    # Define roped and bouldering specific subset\n",
    "    roped_subset = (df_source['Route Type'] == 'Sport') | (df_source['Route Type'] == 'Trad')\n",
    "    boulder_subset = (df_source['Route Type'] == 'Boulder')\n",
    "\n",
    "    ROPED_MIN_LENGTH = 15\n",
    "    ROPED_MAX_LENGTH = 4500 #\"Trango Towers\" are 4,300' tall\n",
    "    ROPED_DEFAULT_LENGTH = 70\n",
    "    BOULDER_MIN_LENGTH = 1\n",
    "    BOULDER_MAX_LENGTH = 55 #\"Too Tall to Fall\" is 50'\n",
    "    BOULDER_DEFAULT_LENGTH = 12\n",
    "\n",
    "    # Fix outliers\n",
    "    def fix_length_outliers(dataframe, subset, minlength, maxlength, deflength):\n",
    "        length_outliers = dataframe[subset][(dataframe[subset]['Length'] <= minlength) | (dataframe[subset]['Length'] >= maxlength)]\n",
    "        for loop_count, (index, data) in enumerate(length_outliers.iterrows()):\n",
    "            if input_type == 'express':\n",
    "                updated_length = deflength\n",
    "            if input_type == 'manual':\n",
    "                updated_length = pyip.inputNum(f\"[{loop_count+1}/{length_outliers.shape[0]}] Outlier Detected, Possible Bad Info. Input Corrected Length for\\nRoute: {data['Route']}\\nLocation: {'>'.join(data['Location'].split('>')[-3:])}\\nCurrently: {data['Length']}ft\\n\", min=minlength, max=maxlength)\n",
    "            dataframe.at[index, 'Length'] = updated_length\n",
    "        return dataframe\n",
    "\n",
    "    # Fill empty route lengths\n",
    "    def fill_length_empties(dataframe, subset, minlength, maxlength, deflength):\n",
    "        length_missing = dataframe[subset][(dataframe[subset]['Length'].isnull()) | (dataframe[subset]['Length'] == 0)]\n",
    "        for loop_count, (index, data) in enumerate(length_missing.iterrows()):\n",
    "            if input_type == 'express':\n",
    "                updated_length = deflength\n",
    "            if input_type == 'manual':\n",
    "                updated_length = pyip.inputNum(f\"[{loop_count+1}/{length_missing.shape[0]}] Input Estimated Length for\\nRoute: {data['Route']}\\nLocation: {'>'.join(data['Location'].split('>')[-3:])}\\n\", min=minlength, max=maxlength)\n",
    "            dataframe.at[index, 'Length'] = updated_length\n",
    "        return dataframe\n",
    "\n",
    "    df_source = fix_length_outliers(df_source, roped_subset, ROPED_MIN_LENGTH, ROPED_MAX_LENGTH, ROPED_DEFAULT_LENGTH)\n",
    "    df_source = fill_length_empties(df_source, roped_subset, ROPED_MIN_LENGTH, ROPED_MAX_LENGTH, ROPED_DEFAULT_LENGTH)\n",
    "    df_source = fix_length_outliers(df_source, boulder_subset, BOULDER_MIN_LENGTH, BOULDER_MAX_LENGTH, BOULDER_DEFAULT_LENGTH)\n",
    "    df_source = fill_length_empties(df_source, boulder_subset, BOULDER_MIN_LENGTH, BOULDER_MAX_LENGTH, BOULDER_DEFAULT_LENGTH)\n",
    "\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_homo(df_source, r_type, r_direction, b_type, b_direction):\n",
    "    \"\"\"\n",
    "    Reassigns grades to a single YDS or Vgrade schema.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_source : df\n",
    "        Original route df.\n",
    "    r_type : str {letter, sign}\n",
    "        YDS letter or sign style grades.\n",
    "    r_direction : str {up, down, even_rand, manual}\n",
    "        Unused if r_type='letter'. Which way to assign grades. even_rand rounds a randomly selected half up and the randomly remaining half down.\n",
    "    b_type : str {flat, sign}\n",
    "        Vgrade flat grades or include sign grades.\n",
    "    b_direction : str {up, down, even_rand, manual}\n",
    "        Same as r_direction\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    df_source : df\n",
    "        Original df with grade homogenization\n",
    "    \"\"\"\n",
    "    grade_homo_choice = ['round down', 'round up', 'round evenly', 'hand determine']\n",
    "    rating_isolate = df_source['Original Rating'].apply(lambda row: [val for val in row.split()][0]) # This is a fail-safe to ensure we are only looking at the part of the rating we care about, not risk or sub-ratings.\n",
    "\n",
    "    # Reset 'Rating' column so this mapping can be re-run\n",
    "    df_source[\"Rating\"] = df_source[\"Original Rating\"]\n",
    "\n",
    "    #Roped Grades\n",
    "    def grademoderate():\n",
    "        grade_change_subset = rating_isolate.isin(list(rgrademoderatemap.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source.loc[grade_change_subset]['Original Rating'].map(rgrademoderatemap)\n",
    "\n",
    "    def grade_split(upmap, downmap):\n",
    "        grade_change_subset = rating_isolate.isin(list(upmap.keys()))\n",
    "        grade_change_subset_df = df_source[grade_change_subset]\n",
    "        for grade in grade_change_subset_df['Original Rating'].unique():\n",
    "            to_change = grade_change_subset_df[grade_change_subset_df['Original Rating'] == grade]\n",
    "            changed_up = to_change.sample(frac=0.5)['Original Rating'].map(upmap)\n",
    "            df_source.loc[changed_up.index, 'Rating'] = changed_up\n",
    "        grade_change_subset = rating_isolate.isin(list(downmap.keys()))\n",
    "        grade_change_subset_df = df_source[grade_change_subset]\n",
    "        for grade in grade_change_subset_df['Original Rating'].unique():\n",
    "            to_change = grade_change_subset_df[grade_change_subset_df['Original Rating'] == grade]\n",
    "            changed_down = to_change['Original Rating'].map(downmap)\n",
    "            df_source.loc[changed_down.index, 'Rating'] = changed_down\n",
    "\n",
    "    if r_type == 'sign':\n",
    "        grade_change_subset = rating_isolate.isin(list(rgradecompmap.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradecompmap)\n",
    "    else:\n",
    "        if r_direction == 'up':\n",
    "            grademoderate()\n",
    "            grade_change_subset = rating_isolate.isin(list(rgradedownmap.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradedownmap)\n",
    "        if r_direction == 'down':\n",
    "            grademoderate()\n",
    "            grade_change_subset = rating_isolate.isin(list(rgradeupmap.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradeupmap)\n",
    "        if r_direction == 'even_rand':\n",
    "            grademoderate()\n",
    "            grade_split(rgradeupmap,rgradedownmap)\n",
    "        if r_direction == 'manual':\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(rgrademoderatemap.keys()) + list(rgradedownmap.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=YDS_GRADES_LETTER)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "\n",
    "    #Boulder Grades\n",
    "    if b_type == 'flat':\n",
    "        # Remove all + and - grades\n",
    "        grade_change_subset = rating_isolate.isin(list(bgradeconmap1.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeconmap1)\n",
    "\n",
    "        if b_direction == 'up':\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradedownmap1.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradedownmap1)\n",
    "        if b_direction == 'down':\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradeupmap1.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeupmap1)\n",
    "        if b_direction == 'even_rand':\n",
    "            grade_split(bgradeupmap1,bgradedownmap1)\n",
    "        if b_direction == 'manual':\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(bgradedownmap1.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=V_GRADES_FLAT)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "\n",
    "    if b_type =='sign':\n",
    "        if b_direction == 'up':\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradedownmap2.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradedownmap2)\n",
    "        if b_direction == 'down':\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradeupmap2.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeupmap2)\n",
    "        if b_direction == 'even_rand':\n",
    "            grade_split(bgradeupmap2,bgradedownmap2)\n",
    "        if b_direction == 'manual':\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(bgradedownmap2.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=V_GRADES_FLAT)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "    \n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_uniq_clean(df_source):\n",
    "    \"\"\"\n",
    "    If a uniq dataframe is constructed from a user tick, it has some irrelevant columns left over that can be removed.\n",
    "    input df, return df.\n",
    "    \"\"\"\n",
    "    col_list = ['Date', 'Notes', 'Your Stars', 'Style', 'Lead Style']\n",
    "    for col in col_list:\n",
    "        if col in df_source.columns:\n",
    "            df_source.drop(columns=col, inplace=True)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_scrape(df_source):\n",
    "    \"\"\"\n",
    "    Downloads the route page and stat page for each entry.\n",
    "    It is suggested you pass this a list of unique routes so it does not download redundantly.\n",
    "    Input df, return df with two new columns of request.Reponse objects.\n",
    "    \"\"\"\n",
    "    res_list = []\n",
    "    if 'Re Mainpage' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Re Mainpage','')\n",
    "    if 'Re Statpage' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Re Statpage','')\n",
    "    \n",
    "    def insert_str_to_address(url, insert_phrase):\n",
    "        str_list = url.split('/')\n",
    "        str_list.insert(4, insert_phrase)\n",
    "        return '/'.join(str_list)\n",
    "\n",
    "    def page_download(url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(url)\n",
    "            print(e)\n",
    "            res = None\n",
    "        res_list.append(res) # Can check this list for errors\n",
    "        \n",
    "        return res\n",
    "\n",
    "    df_source['Re Mainpage'] = df_source['URL'].progress_apply(page_download)\n",
    "    df_source['Re Statpage'] = df_source['URL'].progress_apply(lambda x: page_download(insert_str_to_address(x, 'stats')))\n",
    "\n",
    "    return(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_default_pitch(df_source):\n",
    "    \"\"\"\n",
    "    Analyze the mainpage for listed default pitch lengths.\n",
    "    Necessary for a user's tick export as it includes pitches as their own recorded pitch count. Not required for ToDo exports as it correctly lists the \"official\" pitch count.\n",
    "    Input df, return df with new column of integer type.\n",
    "    \"\"\"\n",
    "    def get_pitches(res):\n",
    "        if res is None:\n",
    "            return None\n",
    "        else:\n",
    "            soup = BeautifulSoup(res.text, 'lxml')\n",
    "            route_type_text = str(soup.find(class_=\"description-details\").find_all('td')[1])\n",
    "            pitch_search = re.search(r'\\d+ pitches',route_type_text)\n",
    "            if str(type(pitch_search)) == \"<class 'NoneType'>\":\n",
    "                num_pitches = 1\n",
    "            else:\n",
    "                num_pitches = pitch_search.group(0).split(' ')[0]\n",
    "            return int(num_pitches)\n",
    "\n",
    "    df_source['Pitches'] = df_source['Re Mainpage'].progress_apply(get_pitches)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tick_details(df_source):\n",
    "    \"\"\"\n",
    "    Extracts a df of tick details for each route from its statpage.\n",
    "    Input df, return df with new column of df type. (Each row has it's own sub-dataframe).\n",
    "    \"\"\"    \n",
    "    def get_tick_details(res):\n",
    "        \"\"\"\n",
    "        Creates a df of tick details from a statpage.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        res : request.response object\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        Username : str\n",
    "            Name of user who ticked\n",
    "        User Link : url\n",
    "            url link of user who ticked\n",
    "        Entry Date : Pandas datetime\n",
    "            date of tick\n",
    "        Pitches ticked : int\n",
    "            Number of pitches in tick\n",
    "        Style : str\n",
    "            Lead, TR, Follow, Attempt etc.\n",
    "        Lead Style : str\n",
    "            Onsight, Flash, Fell/Hung, Redpoint, Pinkpoint etc.\n",
    "        Comment : str\n",
    "            Tick comment\n",
    "        \"\"\"\n",
    "        name = []\n",
    "        namelink = []\n",
    "        entrydate = []\n",
    "        pitches = []\n",
    "        style = []\n",
    "        lead_style = []\n",
    "        comment = []\n",
    "        if res is None:\n",
    "            d = None\n",
    "        else:\n",
    "            soup = BeautifulSoup(res.text, 'lxml')\n",
    "            # print(soup.select(\"#route-stats > div.row.pt-main-content > div > h1\")) # Tells you which page is being scraped, useful for debugging\n",
    "            try:\n",
    "                blocks = list(soup.select(\"#route-stats > div:nth-child(2) > div:nth-last-child(1)\")[0].find_all('tr'))\n",
    "            except:\n",
    "                blocks = []\n",
    "            for x in blocks:\n",
    "                soup = BeautifulSoup(str(x), 'lxml')\n",
    "                entries = soup.find_all('div', attrs={'class': None})\n",
    "                for entry in entries:\n",
    "                    entrytext = entry.text\n",
    "                    try:\n",
    "                        name.append(soup.find('a').text.strip())\n",
    "                    except:\n",
    "                        name.append('')\n",
    "                        \n",
    "                    try:\n",
    "                        namelink.append(soup.find('a')['href'].strip())\n",
    "                    except:\n",
    "                        namelink.append('')\n",
    "                    \n",
    "                    try:\n",
    "                        date_search = [re.search(r'\\w{3}\\s\\d{1,2},\\s\\d{4}', entrytext)]\n",
    "                        entrydate.append([subresult.group(0).strip() if subresult else '' for subresult in date_search][0]) # pulls match text if match object is not none\n",
    "                    except:\n",
    "                        entrydate.append('')\n",
    "                    \n",
    "                    try:\n",
    "                        pitches_search = [re.search(r'·([^.]+\\s(pitches))', entrytext)] # regex for starting at · and ending at first period only if it includes the word \"pitches\"\n",
    "                        pitchesinterm = [subresult.group(0) if subresult else '' for subresult in pitches_search]\n",
    "                        pitches.append([int(re.search(r'\\d+', subresult).group(0).strip()) if subresult else 1 for subresult in pitchesinterm][0]) # take just the digit of the string\n",
    "                    except:\n",
    "                        pitches.append(1)\n",
    "                    \n",
    "                    try:\n",
    "                        style_search = [re.search(r\"(Solo|TR|Follow|Lead|Send|Attempt|Flash)\", entrytext)]\n",
    "                        style_val = [subresult.group(0).strip() if subresult else '' for subresult in style_search][0] # I have a conditional in the comment search that depends on this so I made it a separate variable\n",
    "                        style.append(style_val)\n",
    "                    except:\n",
    "                        style.append('')\n",
    "                    \n",
    "                    try:\n",
    "                        if style_val != '':\n",
    "                            lead_style_search = [re.search(r\"/([^.]+)\", entrytext)]\n",
    "                            lead_style.append([subresult.group(0)[2:].strip() if subresult else '' for subresult in lead_style_search][0])\n",
    "                        else:\n",
    "                            lead_style.append('')\n",
    "                    except:\n",
    "                        lead_style.append('')\n",
    "                    \n",
    "                    try:\n",
    "                        if style_val != '':\n",
    "                            comment_search = [re.search(r\"(Solo|TR|Follow|Lead).*\", entrytext)]\n",
    "                            commentinterm = ([subresult.group(0) if subresult else '' for subresult in comment_search])\n",
    "                            comment.append([re.search(r\"\\..*\", subresult).group(0)[2:].strip() if subresult else '' for subresult in commentinterm][0])\n",
    "                        else:\n",
    "                            comment_search = [re.search(r\"·(.*)\", entrytext)] # If no style comment then entire phrase is the comment.\n",
    "                            comment.append([subresult.group(0)[2:].strip() if subresult else '' for subresult in comment_search][0])\n",
    "                    except:\n",
    "                        comment.append('')\n",
    "            # print (len(name),len(namelink),len(entrydate),len(pitches),len(style),len(lead_style),len(comment))\n",
    "            # print (name,namelink,entrydate,pitches,style,lead_style,comment)\n",
    "            d = pd.DataFrame({'Username' : name, \"User Link\" : namelink, \"Entry Date\": entrydate, \"Pitches Ticked\": pitches, \"Style\": style, \"Lead Style\": lead_style, \"Comment\": comment})\n",
    "            # One last possible error correction, an oomlot injected a \"/\" into lead style and the regex incidentally detected it\n",
    "            d.loc[~d['Lead Style'].isin(TICK_OPTIONS), 'Lead Style'] = ''\n",
    "            # Recast columns to correct data type\n",
    "            d['Entry Date'] = pd.to_datetime(d['Entry Date'], errors = 'coerce')\n",
    "            d['Style'] = pd.Categorical(d['Style'])\n",
    "            d['Lead Style'] = pd.Categorical(d['Lead Style'])\n",
    "            \n",
    "        return d\n",
    "\n",
    "    df_source['Route Ticks']=df_source['Re Statpage'].progress_apply(get_tick_details)\n",
    "    return(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_analysis(df_source):\n",
    "    \"\"\"\n",
    "    Analyzes tick sub-df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_source : df\n",
    "        Source dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Num Ticks : int\n",
    "        Number of ticks\n",
    "    Num Tickers : int\n",
    "        Number of users who ticked\n",
    "    Lead Ratio : float\n",
    "        Ratio of lead ticks to total ticks with non-null style type\n",
    "    OS Ratio : float\n",
    "        Ratio of onsight plus flash ticks to total ticks with non-null lead-style type\n",
    "    Tick Counts : series\n",
    "        series of count of each type of tick\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Analyzes tick sub dataframe to create meaningful metrics.\n",
    "    \n",
    "    def unpack_style(routeticks, colref, pitchnum):\n",
    "        \"\"\"\n",
    "        Returns a flat list of all non-null values in a given tick df column.\n",
    "        pitchnum allows us to handle multipitch ticks differently than singlepitch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        routeticks : df\n",
    "            df of ticks for a specific route\n",
    "        colref : str\n",
    "            column name to unpack from\n",
    "        pitchnum : int\n",
    "            number of pitches in route\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        flat_list : list of strings\n",
    "            Flat list of style strings\n",
    "        \"\"\"\n",
    "        nest_list = []\n",
    "        for row in routeticks.index:\n",
    "            styleval = routeticks[colref][row]\n",
    "            if pitchnum == 1:\n",
    "                if styleval in CLEAN_SEND: # clean sends with multiple ticks are assumed to be fell/hung attempts up to that clean send.\n",
    "                    nest_list.append([routeticks[colref][row]])\n",
    "                    nest_list.append((routeticks['Pitches Ticked'][row]-1)*['Fell/Hung'])\n",
    "                else:\n",
    "                    nest_list.append(routeticks['Pitches Ticked'][row]*[routeticks[colref][row]])\n",
    "            if pitchnum > 1:\n",
    "                nest_list.append([routeticks[colref][row]])\n",
    "        flat_list = [num for sublist in nest_list for num in sublist]\n",
    "        return flat_list\n",
    "\n",
    "    def analyze_tick_counts(routeticks, pitchnum):\n",
    "        if routeticks is None:\n",
    "            num_ticks = num_tickers = lead_ratio = os_ratio = repeat_senders = tick_counts = float('NaN')\n",
    "        else:\n",
    "            # Get number of ticks and tickers\n",
    "            num_ticks = len(routeticks.index)\n",
    "            num_tickers = routeticks['Username'].nunique()\n",
    "            \n",
    "            # Create tick metrics\n",
    "            tick_cat = CategoricalDtype(categories=TICK_OPTIONS)\n",
    "            tick_type_list = pd.Series(unpack_style(routeticks, 'Style', pitchnum) + unpack_style(routeticks, 'Lead Style', pitchnum), dtype=tick_cat)\n",
    "            tick_counts = tick_type_list.value_counts()\n",
    "            repeat_senders = routeticks[routeticks['Lead Style'].isin(CLEAN_SEND)].groupby('Username')['Lead Style'].count().mean() # It is assumed that each clean send gets its own tick.\n",
    "            \n",
    "            \n",
    "            lead_ratio = tick_counts['Lead']/(tick_counts['Follow'] + tick_counts['TR'] + tick_counts['Lead'])\n",
    "            os_ratio = (tick_counts['Onsight'] + tick_counts['Flash']) / (tick_counts['Onsight'] + tick_counts['Flash'] + tick_counts['Fell/Hung'] + tick_counts['Redpoint'] + tick_counts['Pinkpoint'] + tick_counts['Attempt'] + tick_counts['Send'])\n",
    "        return pd.Series([num_ticks, num_tickers, lead_ratio, os_ratio, repeat_senders, tick_counts])\n",
    "\n",
    "    df_source[['Num Ticks', 'Num Tickers', 'Lead Ratio', 'OS Ratio', 'Repeat Sender Ratio', 'Tick Counts']] = df_source.progress_apply(lambda x: analyze_tick_counts(x['Route Ticks'], x['Pitches']), axis=1)\n",
    "    return df_source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69c68274f8e9ce57f21b9f4f0cd989e216f773741f6e1c7e6f7ea408f78a94ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
