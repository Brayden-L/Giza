{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pyinputplus as pyip\n",
    "import requests\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other files\n",
    "%run climbconstants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile = open('../Data_Archive/df_archive_base', 'rb')\n",
    "df_usend_uniq = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_df(upload_type, link):\n",
    "    try:\n",
    "        df = pd.read_csv(f'{link}/{upload_type}-export')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardize(df_source):\n",
    "    ### Standardizes and slims dataset to only include relevant data entries. In particular the \"Route Type\" column which designates what kind of downstream data treatment the entry receives.\n",
    "    ### One of the major\n",
    "    \n",
    "    # Some cleanup is only necessary on the ticked list, and not the todo list.\n",
    "    if 'Date' in df_source.columns:\n",
    "        df_source['Date'] = pd.to_datetime(df_source['Date']) # 'Date' to datetype\n",
    "        df_source.rename(columns={'Pitches': 'Pitches Ticked'}, inplace=True) # Pitches relabeled to Pitches Ticked\n",
    "        df_source['Notes'] = df_source['Notes'].apply(lambda x: str(x).replace('&#39;',\"'\")) # Apostrophe's are html coded in the notes section for some reason.\n",
    "\n",
    "    # Remove all aid, ice, snow, TR only,and trad/boulder climbing route types as they are not relevant.\n",
    "    df_source = df_source[df_source['Route Type'].str.contains(r'Aid|Ice|Snow') != True]\n",
    "    df_source = df_source[df_source['Route Type'].str.fullmatch(r'TR') != True] #if this is just a partial match it will detech \"trad\" too!\n",
    "    df_source = df_source[df_source['Route Type'].str.contains(r'Trad') & df_source['Route Type'].str.contains(r'Boulder') != True]\n",
    "\n",
    "    # \"trad, sport\" goes to \"trad\". If it uses gear it's trad!\n",
    "    df_source.loc[df_source['Route Type'].str.contains(r'Trad') & df_source['Route Type'].str.contains(r'Sport'), 'Route Type'] = 'Trad'\n",
    "\n",
    "    # \"x, alpine\" and \"x, tr\" goes to \"x\" Alpine and tr tags are not useful.\n",
    "    def rem_route_el_from_list(ousted, seperator):\n",
    "        el_rem_subset = df_source['Route Type'].str.contains(ousted) == True\n",
    "        df_source.loc[el_rem_subset, 'Route Type'] = df_source[el_rem_subset]['Route Type'].apply(lambda row: [val for val in row.split(seperator) if val != ousted]).apply(lambda x: \", \".join(x))\n",
    "\n",
    "    rem_route_el_from_list('Alpine', ', ')\n",
    "    rem_route_el_from_list('TR', ', ')\n",
    "\n",
    "    # Extract route unique identifier from URL and create a new column for it.\n",
    "    if 'Route ID' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Route ID','')\n",
    "    df_source['Route ID'] = df_source['URL'].apply(lambda x: x.split('/')[4])\n",
    "\n",
    "    # Change YDS-Vgrade combos to just Vgrade. They are most likely boulders, so a bouldering grade is relevant.\n",
    "    subset = df_source['Rating'].apply(lambda row: [val for val in row.split() if val in V_GRADES_FULL]).astype(bool)  & df_source['Rating'].apply(lambda row: [val for val in row.split() if val in YDS_GRADES_FULL]).astype(bool) == True\n",
    "    df_source.loc[subset, 'Rating'] = df_source[subset]['Rating'].apply(lambda x: x.split()[1])\n",
    "\n",
    "    # Seperate risk rating to new column\n",
    "    if 'Rating' not in df_source.columns:\n",
    "        df_source.insert(df_source.columns.get_loc('Rating')+1,'Risk','')\n",
    "    df_source['Risk'] = df_source['Rating'].apply(lambda row: [val for val in row.split() if val in RISK_GRADES]).apply(lambda x: \"\".join(x))\n",
    "    # Reduce Rating column to just rating\n",
    "    df_source['Rating'] = df_source['Rating'].apply(lambda row: [val for val in row.split()][0])\n",
    "    # Displays all rows where risk ratings have been seperated\n",
    "    # df_usend[df_usend['Risk'].astype(bool)]\n",
    "\n",
    "    # Create original rating and length archive to compare against or undo changes.\n",
    "    if 'Original Rating' not in df_source.columns:\n",
    "        df_source.insert(df_source.columns.get_loc('Rating'),'Original Rating',df_source['Rating'])\n",
    "\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_length_fixer(df_source):\n",
    "### Handle route length outliers and NaNs\n",
    "    # Define roped and bouldering specific subset\n",
    "    roped_subset = (df_source['Route Type'] == 'Sport') | (df_source['Route Type'] == 'Trad')\n",
    "    boulder_subset = (df_source['Route Type'] == 'Boulder')\n",
    "\n",
    "    ROPED_MIN_LENGTH = 15\n",
    "    ROPED_MAX_LENGTH = 4500 #\"Trango Towers\" are 4,300' tall\n",
    "    ROPED_DEFAULT_LENGTH = 70\n",
    "    BOULDER_MIN_LENGTH = 1\n",
    "    BOULDER_MAX_LENGTH = 55 #\"Too Tall to Fall\" is 50'\n",
    "    BOULDER_DEFAULT_LENGTH = 12\n",
    "\n",
    "    # Fix outliers\n",
    "    def fix_length_outliers(dataframe, subset, minlength, maxlength, deflength):\n",
    "        length_outliers = dataframe[subset][(dataframe[subset]['Length'] <= minlength) | (dataframe[subset]['Length'] >= maxlength)]\n",
    "        for loop_count, (index, data) in enumerate(length_outliers.iterrows()):\n",
    "            if skipchoice == 'y':\n",
    "                updated_length = deflength\n",
    "            else:\n",
    "                updated_length = pyip.inputNum(f\"[{loop_count+1}/{length_outliers.shape[0]}] Outlier Detected, Possible Bad Info. Input Corrected Length for\\nRoute: {data['Route']}\\nLocation: {'>'.join(data['Location'].split('>')[-3:])}\\nCurrently: {data['Length']}ft\\n\", min=minlength, max=maxlength)\n",
    "            dataframe.at[index, 'Length'] = updated_length\n",
    "        return dataframe\n",
    "\n",
    "    # Fill empty route lengths\n",
    "    def fill_length_empties(dataframe, subset, minlength, maxlength, deflength):\n",
    "        length_missing = dataframe[subset][(dataframe[subset]['Length'].isnull()) | (dataframe[subset]['Length'] == 0)]\n",
    "        for loop_count, (index, data) in enumerate(length_missing.iterrows()):\n",
    "            if skipchoice == 'y':\n",
    "                updated_length = deflength\n",
    "            else:\n",
    "                updated_length = pyip.inputNum(f\"[{loop_count+1}/{length_missing.shape[0]}] Input Estimated Length for\\nRoute: {data['Route']}\\nLocation: {'>'.join(data['Location'].split('>')[-3:])}\\n\", min=minlength, max=maxlength)\n",
    "            dataframe.at[index, 'Length'] = updated_length\n",
    "        return dataframe\n",
    "\n",
    "    print(F'Would you like to skip manual length assignment and assign a common value of {ROPED_DEFAULT_LENGTH}ft for routes and {BOULDER_DEFAULT_LENGTH}ft for boulders to missing values?')\n",
    "    skipchoice = pyip.inputChoice(['y','n'])\n",
    "    df_source = fix_length_outliers(df_source, roped_subset, ROPED_MIN_LENGTH, ROPED_MAX_LENGTH, ROPED_DEFAULT_LENGTH)\n",
    "    df_source = fill_length_empties(df_source, roped_subset, ROPED_MIN_LENGTH, ROPED_MAX_LENGTH, ROPED_DEFAULT_LENGTH)\n",
    "    df_source = fix_length_outliers(df_source, boulder_subset, BOULDER_MIN_LENGTH, BOULDER_MAX_LENGTH, BOULDER_DEFAULT_LENGTH)\n",
    "    df_source = fill_length_empties(df_source, boulder_subset, BOULDER_MIN_LENGTH, BOULDER_MAX_LENGTH, BOULDER_DEFAULT_LENGTH)\n",
    "\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_homo(df_source):\n",
    "### Homogenize Grades\n",
    "    grade_homo_choice = ['round down', 'round up', 'round evenly', 'hand determine']\n",
    "    rating_isolate = df_source['Original Rating'].apply(lambda row: [val for val in row.split()][0])\n",
    "\n",
    "    # Reset 'Rating' column so this mapping can be re-run\n",
    "    df_source[\"Rating\"] = df_source[\"Original Rating\"]\n",
    "\n",
    "    #Roped Grades\n",
    "    print('ROPED GRADES')\n",
    "    print('Select Roped Grade Homogenization Type:\\n1)5.10a / 5.10b / 5.10c / 5.10d (Requires additional effort)\\n2)5.10- / 5.10 / 5.10+')\n",
    "    r_grade_homo_type = pyip.inputChoice(['1','2'])\n",
    "    if r_grade_homo_type == '1':\n",
    "        r_grade_homo_dir = pyip.inputChoice(grade_homo_choice)\n",
    "\n",
    "\n",
    "    def grademoderate():\n",
    "        grade_change_subset = rating_isolate.isin(list(rgrademoderatemap.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source.loc[grade_change_subset]['Original Rating'].map(rgrademoderatemap)\n",
    "\n",
    "    def grade_split(upmap, downmap):\n",
    "        grade_change_subset = rating_isolate.isin(list(upmap.keys()))\n",
    "        grade_change_subset_df = df_source[grade_change_subset]\n",
    "        for grade in grade_change_subset_df['Original Rating'].unique():\n",
    "            to_change = grade_change_subset_df[grade_change_subset_df['Original Rating'] == grade]\n",
    "            changed_up = to_change.sample(frac=0.5)['Original Rating'].map(upmap)\n",
    "            df_source.loc[changed_up.index, 'Rating'] = changed_up\n",
    "        grade_change_subset = rating_isolate.isin(list(downmap.keys()))\n",
    "        grade_change_subset_df = df_source[grade_change_subset]\n",
    "        for grade in grade_change_subset_df['Original Rating'].unique():\n",
    "            to_change = grade_change_subset_df[grade_change_subset_df['Original Rating'] == grade]\n",
    "            changed_down = to_change['Original Rating'].map(downmap)\n",
    "            df_source.loc[changed_down.index, 'Rating'] = changed_down\n",
    "\n",
    "    if r_grade_homo_type == '2':\n",
    "        grade_change_subset = rating_isolate.isin(list(rgradecompmap.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradecompmap)\n",
    "    else:\n",
    "        if r_grade_homo_dir == grade_homo_choice[0]:\n",
    "            grademoderate()\n",
    "            grade_change_subset = rating_isolate.isin(list(rgradedownmap.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradedownmap)\n",
    "        if r_grade_homo_dir == grade_homo_choice[1]:\n",
    "            grademoderate()\n",
    "            grade_change_subset = rating_isolate.isin(list(rgradeupmap.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(rgradeupmap)\n",
    "        if r_grade_homo_dir == grade_homo_choice[2]:\n",
    "            grademoderate()\n",
    "            grade_split(rgradeupmap,rgradedownmap)\n",
    "        if r_grade_homo_dir == grade_homo_choice[3]:\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(rgrademoderatemap.keys()) + list(rgradedownmap.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=YDS_GRADES_LETTER)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "\n",
    "    #Boulder Grades\n",
    "    print('BOULDER GRADES')\n",
    "    print('Select Boulder Grade Homogenization Type:\\n1)V1\\n2)V1- / V1 / V1+')\n",
    "    b_grade_homo_type = pyip.inputChoice(['1','2'])\n",
    "    b_grade_homo_dir = pyip.inputChoice(grade_homo_choice)\n",
    "\n",
    "\n",
    "    if b_grade_homo_type == '1':\n",
    "        # Remove all + and - grades\n",
    "        grade_change_subset = rating_isolate.isin(list(bgradeconmap1.keys()))\n",
    "        df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeconmap1)\n",
    "\n",
    "        if b_grade_homo_dir == grade_homo_choice[0]:\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradedownmap1.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradedownmap1)\n",
    "        if b_grade_homo_dir == grade_homo_choice[1]:\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradeupmap1.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeupmap1)\n",
    "        if b_grade_homo_dir == grade_homo_choice[2]:\n",
    "            grade_split(bgradeupmap1,bgradedownmap1)\n",
    "        if b_grade_homo_dir == grade_homo_choice[3]:\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(bgradedownmap1.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=V_GRADES_BASIC)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "\n",
    "    if b_grade_homo_type =='2':\n",
    "        if b_grade_homo_dir == grade_homo_choice[0]:\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradedownmap2.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradedownmap2)\n",
    "        if b_grade_homo_dir == grade_homo_choice[1]:\n",
    "            grade_change_subset = rating_isolate.isin(list(bgradeupmap2.keys()))\n",
    "            df_source.loc[grade_change_subset, 'Rating'] = df_source[grade_change_subset]['Original Rating'].map(bgradeupmap2)\n",
    "        if b_grade_homo_dir == grade_homo_choice[2]:\n",
    "            grade_split(bgradeupmap2,bgradedownmap2)\n",
    "        if b_grade_homo_dir == grade_homo_choice[3]:\n",
    "            needs_grade_corr = df_source[rating_isolate.isin(list(bgradedownmap2.keys()))]\n",
    "            for loop_count, (index, data) in enumerate(needs_grade_corr.iterrows()):\n",
    "                updated_grade = pyip.inputChoice(prompt=f\"[{loop_count+1}/{needs_grade_corr.shape[0]}] Input Grade Correction For: {data['Route'].title()}:\\n\", choices=V_GRADES_BASIC)\n",
    "                df_source.at[index, 'Rating'] = updated_grade\n",
    "    \n",
    "    grade_settings = {'r_grade_homo_type': r_grade_homo_type, 'r_grade_homo_dir': r_grade_homo_dir, 'b_grade_homo_type': b_grade_homo_type, 'b_grade_homo_dir': b_grade_homo_dir}\n",
    "    \n",
    "    return df_source, grade_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_uniq_clean(df_source):\n",
    "    ### Cleans up uniq dataframe of userdataframe specific columns that are not relevant.\n",
    "    col_list = ['Date', 'Notes', 'Your Stars', 'Style', 'Lead Style']\n",
    "    for col in col_list:\n",
    "        if col in df_source.columns:\n",
    "            df_source.drop(columns=col, inplace=True)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_scrape(df_source):\n",
    "    ### Downloads the route page and stat page for each entry. It is suggested you pass this a list of unique routes so it does not download redundantly.\n",
    "    \n",
    "    res_list = []\n",
    "    if 'Re mainpage' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Re mainpage','')\n",
    "    if 'Re statpage' not in df_source.columns:\n",
    "        df_source.insert(len(df_source.columns),'Re statpage','')\n",
    "    \n",
    "    def insert_str_to_address(url, insert_phrase):\n",
    "        str_list = url.split('/')\n",
    "        str_list.insert(4, insert_phrase)\n",
    "        return '/'.join(str_list)\n",
    "\n",
    "    def page_download(url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            res = ''\n",
    "        res_list.append(res) # Can check this list for errors\n",
    "        \n",
    "        return res\n",
    "\n",
    "    df_source['Re Mainpage'] = df_source['URL'].progress_apply(page_download)\n",
    "    df_source['Re Statpage'] = df_source['URL'].progress_apply(lambda x: page_download(insert_str_to_address(x, 'stats')))\n",
    "\n",
    "    return(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_default_pitch(df_source):\n",
    "    ### Analyze the mainpage for listed default pitch lengths. Necessary for a user's tick export as it includes pitches as their own recorded pitch count. Not required for ToDo exports as it correctly lists the \"official\" pitch count.\n",
    "\n",
    "    def get_pitches(res):\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        route_type_text = str(soup.find(class_=\"description-details\").find_all('td')[1])\n",
    "        pitch_search = re.search(r'\\d+ pitches',route_type_text)\n",
    "        if str(type(pitch_search)) == \"<class 'NoneType'>\":\n",
    "            num_pitches = 1\n",
    "        else:\n",
    "            num_pitches = pitch_search.group(0).split(' ')[0]\n",
    "        return int(num_pitches)\n",
    "\n",
    "    df_source['Pitches'] = df_source['Re Mainpage'].progress_apply(get_pitches)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tick_details(df_source):\n",
    "    ### From a routes statpage, this analyzes and orders all ticks into a sub-dataframe that can be referenced for further analysis\n",
    "    \n",
    "    def get_tick_details(res):\n",
    "\n",
    "        name = []\n",
    "        namelink = []\n",
    "        entrydate = []\n",
    "        pitches = []\n",
    "        style = []\n",
    "        lead_style = []\n",
    "        comment = []\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # print(soup.select(\"#route-stats > div.row.pt-main-content > div > h1\")) # Tells you which page is being scraped, useful for debugging\n",
    "        try:\n",
    "            blocks = list(soup.select(\"#route-stats > div:nth-child(2) > div:nth-last-child(1)\")[0].find_all('tr'))\n",
    "        except:\n",
    "            blocks = []\n",
    "        for x in blocks:\n",
    "            soup = BeautifulSoup(str(x), 'html.parser')\n",
    "            entries = soup.find_all('div', attrs={'class': None})\n",
    "            for entry in entries:\n",
    "                entrytext = entry.text\n",
    "                try:\n",
    "                    name.append(soup.find('a').text.strip())\n",
    "                except:\n",
    "                    name.append('')\n",
    "                    \n",
    "                try:\n",
    "                    namelink.append(soup.find('a')['href'].strip())\n",
    "                except:\n",
    "                    namelink.append('')\n",
    "                \n",
    "                try:\n",
    "                    date_search = [re.search(r'\\w{3}\\s\\d{1,2},\\s\\d{4}', entrytext)]\n",
    "                    entrydate.append([subresult.group(0).strip() if subresult else '' for subresult in date_search][0]) # pulls match text if match object is not none\n",
    "                except:\n",
    "                    entrydate.append('')\n",
    "                \n",
    "                try:\n",
    "                    pitches_search = [re.search(r'·([^.]+\\s(pitches))', entrytext)] # regex for starting at · and ending at first period only if it includes the word \"pitches\"\n",
    "                    pitchesinterm = [subresult.group(0) if subresult else '' for subresult in pitches_search]\n",
    "                    pitches.append([int(re.search(r'\\d+', subresult).group(0).strip()) if subresult else 1 for subresult in pitchesinterm][0]) # take just the digit of the string\n",
    "                except:\n",
    "                    pitches.append(1)\n",
    "                \n",
    "                try:\n",
    "                    style_search = [re.search(r\"(Solo|TR|Follow|Lead|Send|Attempt|Flash)\", entrytext)]\n",
    "                    style_val = [subresult.group(0).strip() if subresult else '' for subresult in style_search][0] # I have a conditional in the comment search that depends on this so I made it a separate variable\n",
    "                    style.append(style_val)\n",
    "                except:\n",
    "                    style.append('')\n",
    "                \n",
    "                try:\n",
    "                    if style_val != '':\n",
    "                        lead_style_search = [re.search(r\"/([^.]+)\", entrytext)]\n",
    "                        lead_style.append([subresult.group(0)[2:].strip() if subresult else '' for subresult in lead_style_search][0])\n",
    "                    else:\n",
    "                        lead_style.append('')\n",
    "                except:\n",
    "                    lead_style.append('')\n",
    "                \n",
    "                try:\n",
    "                    if style_val != '': \n",
    "                        comment_search = [re.search(r\"(Solo|TR|Follow|Lead).*\", entrytext)]\n",
    "                        commentinterm = ([subresult.group(0) if subresult else '' for subresult in comment_search])\n",
    "                        comment.append([re.search(r\"\\..*\", subresult).group(0)[2:].strip() if subresult else '' for subresult in commentinterm][0])\n",
    "                    else:\n",
    "                        comment_search = [re.search(r\"·(.*)\", entrytext)] # If no style comment then entire phrase is the comment.\n",
    "                        comment.append([subresult.group(0)[2:].strip() if subresult else '' for subresult in comment_search][0])\n",
    "                except:\n",
    "                    comment.append('')\n",
    "        # print (len(name),len(namelink),len(entrydate),len(pitches),len(style),len(lead_style),len(comment))\n",
    "        # print (name,namelink,entrydate,pitches,style,lead_style,comment)\n",
    "        d = pd.DataFrame({'Username' : name, \"User Link\" : namelink, \"Entry Date\": entrydate, \"Pitches\": pitches, \"Style\": style, \"Lead Style\": lead_style, \"Comment\": comment})\n",
    "        # One last possible error correction, an oomlot injected a \"/\" into lead style and the regex incidentally detected it\n",
    "        d.loc[~d['Lead Style'].isin(TICK_OPTIONS), 'Lead Style'] = ''\n",
    "        return d\n",
    "\n",
    "    df_source['Route Ticks']=df_source['Re Statpage'].progress_apply(get_tick_details)\n",
    "    return(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_analysis(df_source):\n",
    "    ### Analyzes tick sub dataframe to create meaningful metrics.\n",
    "    \n",
    "    def unpack_style(statpage_df, colref):\n",
    "        nest_list = []\n",
    "        for row in statpage_df.index:\n",
    "            styleval = statpage_df[colref][row]\n",
    "            if styleval in CLEAN_SEND: # clean sends with multiple ticks are assumed to be fell/hung attempts up to that clean send.\n",
    "                nest_list.append([statpage_df[colref][row]])\n",
    "                nest_list.append((statpage_df['Pitches'][row]-1)*['Fell/Hung'])\n",
    "            else:\n",
    "                nest_list.append(statpage_df['Pitches'][row]*[statpage_df[colref][row]])\n",
    "        flat_list = [num for sublist in nest_list for num in sublist]\n",
    "        return flat_list\n",
    "\n",
    "    def analyze_tick_counts(statpagedf):\n",
    "        # Get number of ticks and tickers\n",
    "        num_ticks = len(statpagedf.index)\n",
    "        num_tickers = statpagedf['Username'].nunique()\n",
    "        \n",
    "        # Create tick metrics\n",
    "        search_counts_init = pd.Series(np.nan, index=TICK_OPTIONS).to_frame()\n",
    "        tick_type_list = unpack_style(statpagedf, 'Style') + unpack_style(statpagedf, 'Lead Style')\n",
    "        tick_type_list = list(filter(None, tick_type_list))\n",
    "        search_counts = pd.Series(tick_type_list).value_counts().dropna().to_frame()\n",
    "        search_counts = search_counts_init.fillna(search_counts).fillna(0).squeeze()\n",
    "        lead_ratio = search_counts['Lead']/(search_counts['Follow'] + search_counts['TR'] + search_counts['Lead'])\n",
    "        os_ratio = (search_counts['Onsight'] + search_counts['Flash']) / (search_counts['Onsight'] + search_counts['Flash'] + search_counts['Fell/Hung'] + search_counts['Redpoint'] + search_counts['Pinkpoint'] + search_counts['Attempt'] + search_counts['Send'])\n",
    "        return pd.Series([num_ticks, num_tickers, lead_ratio, os_ratio])\n",
    "\n",
    "    df_source[['Num Ticks', 'Num Tickers', 'Lead Ratio', 'OS Ratio']] = df_source['Route Ticks'].apply(analyze_tick_counts)\n",
    "    return df_source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69c68274f8e9ce57f21b9f4f0cd989e216f773741f6e1c7e6f7ea408f78a94ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
